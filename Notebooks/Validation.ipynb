{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the validation step the trained model and pipeline was tested on data that the model has not seen before. The metric used to examine the results were ROC, accuracy, and f1 score. Also, recall and precision for one and zero was examined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pyspark using Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('val').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('clean_val/part-00000-2661d739-2781-4738-9b1b-6b4c69096d9d-c000.csv', header = True).select('Text', 'verified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                Text|verified|\n",
      "+--------------------+--------+\n",
      "|   really good movie|    true|\n",
      "|review didnt like...|    true|\n",
      "|shabby zombieposs...|    true|\n",
      "|disturbing good a...|    true|\n",
      "|                null|    true|\n",
      "|love plot story l...|    true|\n",
      "|great revenge mov...|    true|\n",
      "|          worth time|    true|\n",
      "|         great movie|    true|\n",
      "|great movie inter...|    true|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### View data\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Text: 21625\n",
      "Null verified: 0\n"
     ]
    }
   ],
   "source": [
    "#### look for nan values \n",
    "print('Null Text:', df.where((df[\"Text\"].isNull())).count())\n",
    "print('Null verified:', df.where((df[\"verified\"].isNull())).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1729882"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### drop na's\n",
    "df = df.na.drop()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Text|label|\n",
      "+--------------------+-----+\n",
      "|   really good movie|  1.0|\n",
      "|review didnt like...|  1.0|\n",
      "|shabby zombieposs...|  1.0|\n",
      "|disturbing good a...|  1.0|\n",
      "|love plot story l...|  1.0|\n",
      "|great revenge mov...|  1.0|\n",
      "|          worth time|  1.0|\n",
      "|         great movie|  1.0|\n",
      "|great movie inter...|  1.0|\n",
      "|        agreed titty|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### create a Label column\n",
    "df = df.withColumn('label', when(df.verified == 'true', 1.0).otherwise(0.0)).select('Text', 'label')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pipeline & Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline and trained model were imported in to be tested on the validation data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import pipeline \n",
    "from pyspark.ml import PipelineModel, Pipeline\n",
    "load_pipline = PipelineModel.read().load('pipline_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import model \n",
    "model = LogisticRegressionModel.load('LGmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|                Text|label|          token_text|         rawFeatures|                 idf|            features|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|   really good movie|  1.0|[really, good, mo...|(262144,[0,3,9],[...|(262144,[0,3,9],[...|(262144,[0,3,9],[...|\n",
      "|review didnt like...|  1.0|[review, didnt, l...|(262144,[4,57,59,...|(262144,[4,57,59,...|(262144,[4,57,59,...|\n",
      "|shabby zombieposs...|  1.0|[shabby, zombiepo...|(262144,[0,56,87,...|(262144,[0,56,87,...|(262144,[0,56,87,...|\n",
      "|disturbing good a...|  1.0|[disturbing, good...|(262144,[3,47,114...|(262144,[3,47,114...|(262144,[3,47,114...|\n",
      "|love plot story l...|  1.0|[love, plot, stor...|(262144,[5,7,56,6...|(262144,[5,7,56,6...|(262144,[5,7,56,6...|\n",
      "|great revenge mov...|  1.0|[great, revenge, ...|(262144,[0,2,4,9,...|(262144,[0,2,4,9,...|(262144,[0,2,4,9,...|\n",
      "|          worth time|  1.0|       [worth, time]|(262144,[6,68],[1...|(262144,[6,68],[1...|(262144,[6,68],[1...|\n",
      "|         great movie|  1.0|      [great, movie]|(262144,[0,2],[1....|(262144,[0,2],[0....|(262144,[0,2],[0....|\n",
      "|great movie inter...|  1.0|[great, movie, in...|(262144,[0,2,5,23...|(262144,[0,2,5,23...|(262144,[0,2,5,23...|\n",
      "|        agreed titty|  0.0|     [agreed, titty]|(262144,[4061,346...|(262144,[4061,346...|(262144,[4061,346...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val = load_pipline.transform(df)\n",
    "val.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  1.0|       1.0|[0.16254599207776...|\n",
      "|  1.0|       1.0|[0.17328587700396...|\n",
      "|  1.0|       1.0|[0.20860411877465...|\n",
      "|  1.0|       1.0|[0.17162704128013...|\n",
      "|  1.0|       1.0|[0.17139272015373...|\n",
      "|  1.0|       1.0|[0.20543779656975...|\n",
      "|  1.0|       1.0|[0.16300693263487...|\n",
      "|  1.0|       1.0|[0.15696064858238...|\n",
      "|  1.0|       1.0|[0.20239410797914...|\n",
      "|  0.0|       1.0|[0.16685376261038...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.transform(val)\n",
    "pred.select('label', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric used were ROC, accuracy, and f1 score. All three metric showed results that were better than the results on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.8001782387143311\n"
     ]
    }
   ],
   "source": [
    "#### R0C\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8516534653808757\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#### Accuracy \n",
    "acc = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "print('Accuracy:', acc.evaluate(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8217714404524895\n"
     ]
    }
   ],
   "source": [
    "#### F1 Score \n",
    "ff = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "print('F1 score:', ff.evaluate(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall , Precision, F1 score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the recall and precision for both zero and one the recall for zero is not great. The model only gets about 0.25 of the actual zero(false) right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics as skmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.25      0.36    294987\n",
      "         1.0       0.86      0.98      0.92   1434895\n",
      "\n",
      "    accuracy                           0.85   1729882\n",
      "   macro avg       0.77      0.61      0.64   1729882\n",
      "weighted avg       0.83      0.85      0.82   1729882\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Classification Report \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
