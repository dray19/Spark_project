{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing  Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step of the project we cleaned the data. We removed punctuations, special characters, and lowercased all words first, then tokenized the words for removing of stop words. Next the words were stemmed using lemmatization. Next the words that were not longer than 4 were removed. The last step was to split the data into train, validation, and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pyspark using Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('cleaning').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('Movies_and_TV.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|          reviewText|verified|\n",
      "+--------------------+--------+\n",
      "|really happy they...|    true|\n",
      "|Having lived in W...|    true|\n",
      "|Excellent look in...|   false|\n",
      "|More than anythin...|    true|\n",
      "|This is a great m...|    true|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### View Data\n",
    "df = df.select('reviewText', 'verified')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8757545"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### droping na values \n",
    "df = df.na.drop()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations & special characters & lowercase words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step punctuations, special characters, and numbers were removed. Also, all words were now lowercased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean Function \n",
    "def clean_text(c):\n",
    "    c = lower(c)\n",
    "    c = regexp_replace(c, \"^rt \", \"\")\n",
    "    c = regexp_replace(c, \"[\\=.]\",\" \")\n",
    "    c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "    c = regexp_replace(c, \"  \", \" \")\n",
    "    c = regexp_replace(c, \"   \", \" \")\n",
    "    c = regexp_replace(c, '\\d+', \"\")\n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+\n",
      "|          reviewText|verified|          clean_text|\n",
      "+--------------------+--------+--------------------+\n",
      "|really happy they...|    true|really happy they...|\n",
      "|Having lived in W...|    true|having lived in w...|\n",
      "|Excellent look in...|   false|excellent look in...|\n",
      "|More than anythin...|    true|more than anythin...|\n",
      "|This is a great m...|    true|this is a great m...|\n",
      "|This movie was in...|    true|this movie was in...|\n",
      "|This is a fascina...|    true|this is a fascina...|\n",
      "|This DVD appears ...|    true|this dvd appears ...|\n",
      "|This movie is not...|    true|this movie is not...|\n",
      "|So sorry I didn't...|    true|so sorry i didnt ...|\n",
      "|Product received ...|    true|product received ...|\n",
      "|Believe me when I...|    true|believe me when i...|\n",
      "|This video arrive...|    true|this video arrive...|\n",
      "|The Reunion of th...|   false|the reunion of th...|\n",
      "|Wedding Music (3:...|   false|wedding music  ge...|\n",
      "|This is truly a m...|   false|this is truly a m...|\n",
      "|It is an excellen...|   false|it is an excellen...|\n",
      "|I have a thing ag...|    true|i have a thing ag...|\n",
      "|This DVD is unbel...|   false|this dvd is unbel...|\n",
      "|Just brought this...|    true|just brought this...|\n",
      "+--------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### View Clean \n",
    "df = df.withColumn(\"clean_text\",clean_text(col('reviewText')))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|verified|          token_text|\n",
      "+--------+--------------------+\n",
      "|    true|[really, happy, t...|\n",
      "|    true|[having, lived, i...|\n",
      "|   false|[excellent, look,...|\n",
      "|    true|[more, than, anyt...|\n",
      "|    true|[this, is, a, gre...|\n",
      "|    true|[this, movie, was...|\n",
      "|    true|[this, is, a, fas...|\n",
      "|    true|[this, dvd, appea...|\n",
      "|    true|[this, movie, is,...|\n",
      "|    true|[so, sorry, i, di...|\n",
      "|    true|[product, receive...|\n",
      "|    true|[believe, me, whe...|\n",
      "|    true|[this, video, arr...|\n",
      "|   false|[the, reunion, of...|\n",
      "|   false|[wedding, music, ...|\n",
      "|   false|[this, is, truly,...|\n",
      "|   false|[it, is, an, exce...|\n",
      "|    true|[i, have, a, thin...|\n",
      "|   false|[this, dvd, is, u...|\n",
      "|    true|[just, brought, t...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### tokenize words \n",
    "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"token_text\")\n",
    "token = tokenizer.transform(df).select('verified', 'token_text')\n",
    "token.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words were removed using the pyspark ml feature “StopWordsRemover”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|verified|            swr_text|\n",
      "+--------+--------------------+\n",
      "|    true|[really, happy, g...|\n",
      "|    true|[lived, west, new...|\n",
      "|   false|[excellent, look,...|\n",
      "|    true|[anything, ive, c...|\n",
      "|    true|[great, movie, mi...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### remove stops words \n",
    "remover = StopWordsRemover(inputCol='token_text', outputCol='swr_text')\n",
    "swr = remover.transform(token).select('verified','swr_text')\n",
    "swr.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lemmatization step the python library “nltk” was used. A function was created to stem the words down using the “WordNetLemmatizer” from the “nltk” library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Function \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Instantiate stemmer object\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "def lem(in_vec):\n",
    "    out_vec = []\n",
    "    for t in in_vec:\n",
    "        t_stem = lemmer.lemmatize(t)\n",
    "        if len(t_stem) > 2:\n",
    "            out_vec.append(t_stem)       \n",
    "    return(out_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|verified|                 lem|\n",
      "+--------+--------------------+\n",
      "|    true|[really, happy, g...|\n",
      "|    true|[lived, west, new...|\n",
      "|   false|[excellent, look,...|\n",
      "|    true|[anything, ive, c...|\n",
      "|    true|[great, movie, mi...|\n",
      "|    true|[movie, english, ...|\n",
      "|    true|[fascinating, tru...|\n",
      "|    true|[dvd, appears, ge...|\n",
      "|    true|[movie, english, ...|\n",
      "|    true|[sorry, didnt, pu...|\n",
      "|    true|[product, receive...|\n",
      "|    true|[believe, tell, r...|\n",
      "|    true|[video, arrived, ...|\n",
      "|   false|[reunion, cathedr...|\n",
      "|   false|[wedding, music, ...|\n",
      "|   false|[truly, moving, v...|\n",
      "|   false|[excellent, exper...|\n",
      "|    true|[thing, purchasin...|\n",
      "|   false|[dvd, unbelievabl...|\n",
      "|    true|[brought, dvd, ho...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Use function \n",
    "from pyspark.sql.types import *\n",
    "lemmer_udf = udf(lambda x: lem(x), ArrayType(StringType()))\n",
    "\n",
    "lem_text = swr.withColumn(\"lem\", lemmer_udf(col(\"swr_text\"))).select('verified', 'lem')\n",
    "lem_text.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Short words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last cleaning step, we removed words that were not longer than four characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                Text|verified|\n",
      "+--------------------+--------+\n",
      "|really happy evan...|    true|\n",
      "|lived west guinea...|    true|\n",
      "|excellent look co...|   false|\n",
      "|anything challeng...|    true|\n",
      "|great movie missi...|    true|\n",
      "|movie english gre...|    true|\n",
      "|fascinating true ...|    true|\n",
      "|appears german en...|    true|\n",
      "|movie english alt...|    true|\n",
      "|sorry didnt purch...|    true|\n",
      "|product received ...|    true|\n",
      "|believe tell rece...|    true|\n",
      "|video arrived per...|    true|\n",
      "|reunion cathedral...|   false|\n",
      "|wedding music fre...|   false|\n",
      "|truly moving vide...|   false|\n",
      "|excellent experie...|   false|\n",
      "|thing purchasing ...|    true|\n",
      "|unbelievable punk...|   false|\n",
      "|brought home rock...|    true|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Removing words \n",
    "filter_length_udf = udf(lambda row: \" \".join([x for x in row if len(x) >= 4]))\n",
    "df2= lem_text.withColumn('Text', filter_length_udf(col('lem'))).select('Text', 'verified')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a index to split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An index was added to the dataset because when we used the random split function in pyspark the text column would turn into null values. To find another way to split the data we created an index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----+\n",
      "|                Text|verified|index|\n",
      "+--------------------+--------+-----+\n",
      "|really happy evan...|    true|    0|\n",
      "|lived west guinea...|    true|    1|\n",
      "|excellent look co...|   false|    2|\n",
      "|anything challeng...|    true|    3|\n",
      "|great movie missi...|    true|    4|\n",
      "|movie english gre...|    true|    5|\n",
      "|fascinating true ...|    true|    6|\n",
      "|appears german en...|    true|    7|\n",
      "|movie english alt...|    true|    8|\n",
      "|sorry didnt purch...|    true|    9|\n",
      "+--------------------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn('index', monotonically_increasing_id())\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data to Train, Val, Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data on a 80% train and 20% validation split with 1 row for the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 7006036.0\n",
      "Val: 1751509.0\n"
     ]
    }
   ],
   "source": [
    "### 80/20 split \n",
    "print('Train:',8757545 * .80 )\n",
    "print('Val:',8757545 * .20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7006037"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train set\n",
    "train = df2.filter(df2.index <= 7006036)\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1751507"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Validation set\n",
    "val = df2.filter((df2.index > 7006036) & (df2.index < 8757544))\n",
    "val.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test example\n",
    "test = df2.filter(df2.index == 8757544)\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step was to export the data for the next three steps in the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export validation set\n",
    "val.write.option(\"header\", \"true\").csv('clean_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export train set\n",
    "train.write.option(\"header\", \"true\").csv('clean_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export test set\n",
    "test.write.option(\"header\", \"true\").csv('clean_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
